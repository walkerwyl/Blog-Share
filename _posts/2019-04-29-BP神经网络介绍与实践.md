---
layout:     post
title:      "BP神经网络介绍及实践"
date:       2019-04-25
author:     "林柯秀"
header-img: "img/post-bg-2015.jpg"
tags:
    - 神经网络
    - Python
    - 手写识别
---

## 一、神经网络产生及发展

1. 西班牙神经组织学家 Santiago Ramón y Cajal （ 1852~1934 ）发现神经细胞之间没有原生质的联系，因而提出神经细胞是整个神经活动最基本的单位（故称神经元），从而使复杂的神经系统有了进一步研究的切入口。他对于大脑的微观结构研究是开创性的，被许多人认为是现代神经科学之父。

2. 艾伦图灵在他的文章《COMPUTING MACHINERY AND INTELLIGENCE》中提出了几个标准来评估一台机器是否可以被认为是智能的，被称为“图灵测试”。神经元及其连接里也许藏着智能的隐喻，沿着这条路线前进的人被称为连接主义。

3. 1943年，Warren McCulloch 和 Walter Pitts 发表题为《A Logical Calculus of the Ideas Immanent in Nervous Activity》的论文，首次提出神经元的M-P模型。该模型借鉴了已知的神经细胞生物过程原理，是第一个神经元数学模型，是人类历史上第一次对大脑工作原理描述的尝试。

- M-P模型的工作原理是神经元的输入信号加权求和，与阈值比较再决定神经元是否输出。这是从原理上证明了人工神经网络可以计算任何算术和逻辑函数。

![1556468293026](/Blog-Share/img/1904/04/shuangmulin/1556468293026.png)

4. 20世纪40年代末，Donald Olding Hebb在《The Organization of Behavior》中对神经元之间连接强度的变化进行了分析，首次提出来一种调整权值的方法，称为Hebb学习规则。

- Hebb学习规则主要假定机体的行为可以由神经元的行为来解释。Hebb受启发于巴普罗夫的条件反射实验，认为如果两个神经元在同一时刻被激发，则它们之间的联系应该被强化。这就是Hebb提出的生物神经元的学习机制，在这种学习中，由对神经元的重复刺激，使得神经元之间的突触强度增加。

- Hebb学习规则隶属于无监督学习算法的范畴，其主要思想是根据两个神经元的激发状态来调整期连接关系，以此实现对简单神经活动的模拟。继Hebb学习规则之后，神经元的有监督Delta学习规则被提出，用于解决在输入输出已知的情况下神经元权值的学习问题。

5. 1958年，就职于Cornell航空实验室的Frank Rosenblatt发明了的一种称为感知器（Perceptron）的人工神经网络。它可以被视为一种最简单形式的前馈神经网络，是一种二元线性分类器（激活函数为sign(x)）。感知机是人工神经网络的第一个实际应用，标志着神经网络进入了新的发展阶段。

6. 1960年，斯坦福大学教授Bernard Widrow教授和他的研究生Ted Hoff开发了Adaline（Adaptive Linear Neuron 或  Adaptive Linear Element）和最小均方滤波器（LMS）。Adaline网络和感知机的区别就是将感知机的Step函数换为Linear线性函数。

7. 美国加州大学的鲁梅尔哈特和麦克莱兰在研究并行分布式信息处理方法，探索人类认知微结构的过程中，于1985年提出BP网络模型。

8. 1988年，继BP算法之后，David Broomhead 和 David Lowe 将径向基函数引入到神经网络的设计中，形成了径向基神经网络（RBF）。RBF网络是神经网络真正走向实用化的一个重要标志。

9. 2006年， G. E. Hinton 和他的学生 R. R. Salakhutdinov 在《科学》杂志上发表题为《Reducing the Dimensionality of Data with Neural Networks》的文章，掀起了深度学习在学术界和工业界的研究热潮。文章摘要阐述了两个重要观点：一是多隐层的神经网络可以学习到能刻画数据本质属性的特征，对数据可视化和分类等任务有很大帮助；二是可以借助于无监督的“逐层初始化”策略来有效克服深层神经网络在训练上存在的难度。

10. 从感知机提出，到BP算法应用以及2006年以前的历史被称为浅层学习，以后的历史被称为深度学习。

## 二、BP神经网络算法

BP神经网络是一种多层的前馈神经网络，其主要的特点是：**信号是前向传播的，而误差是反向传播的。**具体来说，对于如下的只含一个隐层的神经网络模型。

BP神经网络的过程主要分为两个阶段:

- 第一阶段是信号的前向传播，从输入层经过隐含层，最后到达输出层；
- 第二阶段是误差的反向传播，从输出层到隐含层，最后到输入层，依次调节隐含层到输出层的权重和偏置，输入层到隐含层的权重和偏置。

神经网络的基本组成单元是神经元。神经元的通用模型如图所示。

![1556468316800](/Blog-Share/img/1904/04/shuangmulin/1556468316800.png)

其中常用的激活函数有阈值函数、sigmoid函数和双曲正切函数，神经元的输出为：
$$
y=f(\sum_{i=1}^m\omega_ix_i)
$$
一个神经网络包括**输入层、隐含层（中间层）和输出层**。**输入层神经元个数与输入数据的维数相同，输出层神经元个数与需要拟合的数据个数相同，隐含层神经元个数与层数需要设计者自己根据一些规则和目标来设定。**在深度学习出现之前，隐含层的层数通常为一层，即通常使用的神经网络是3层网络。

![1556468331194](assets/1556468331194.png)

### 2.1 激活函数

类比生物神经元，它接收一个电输入，输出另一个电信号。观察表明，神经元不会立即反应，而是会抑制输入，直到输入增强，强大到可以触发输出。即产生输出之前，输入必须到达一个阈值——直观的感觉在于，神经元不希望传递微小的噪声信号，而只是传递有意识的明显信号。激活函数的使用可以从数学上表示这一过程。

激活函数必须处处可导，且一般使用一般都使用S型函数。S函数有单极性S型函数和双极性S型函数两种，单极性S型函数定义及图示如下：
$$
f(x)=\frac{1}{1+e^{-x}}
$$
![1556468358923](/Blog-Share/img/1904/04/shuangmulin/1556468358923.png)

双极性S型函数定义如下：
$$
f(x)=\frac{1-e^{-x}}{1+e^{-x}}
$$
![1556468371026](/Blog-Share/img/1904/04/shuangmulin/1556468371026.png)

在进行神经网络训练时，应该将输入控制在sigmoid函数收敛较快的区间范围内。

### 2.2 BP神经网络的学习过程

![1556468420566](/Blog-Share/img/1904/04/shuangmulin/1556468420566.png)

### 2.3 如何理解学习率

BP神经网络的本质就是对全局误差求最小，就像是在求误差的“山谷”，学习率相当于在进行梯度下降时模型所采用的试探步长。故当误差对权值的偏导大于零时，权值调整量为负；当误差对权值的偏导小于零时，权值调整量为正。

## 三、使用BP神经网络进行手写识别

使用MNIST的手写数字训练集（知名数据集，可自行下载）对构建的3层BP神经网络进行训练，并在后续优化过程中调整各种参数，以求获得更佳的模型表现。最后使模型成功达到了97.5%的识别准确率。

python实现代码如下：

```python
import numpy
import scipy.special
import matplotlib.pyplot
%matplotlib inline

# 创建神经网络对象
class neuralNetwork:
    # 初始化一个神经网络
    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):
        self.inodes = inputnodes
        self.hnodes = hiddennodes
        self.onodes = outputnodes
        
        # 正态概率分布采样权重，其中平均值为0，标准方差为下一层节点数的开方倒数，并定义wih的大小
        self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))
        self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))
        
        self.lr = learningrate
        
        self.activation_function = lambda x: scipy.special.expit(x)
        
    #训练神经网络   
    def train(self, inputs_num, targets_num):
        # 将输入的数值转换为二维数组，形成矩阵
        inputs = numpy.array(inputs_num, ndmin=2).T
        targets = numpy.array(targets_num, ndmin=2).T
        
        #计算隐藏层输入输出
        hidden_inputs = numpy.dot(self.wih, inputs)
        hidden_outputs = self.activation_function(hidden_inputs)
        
        #计算输出层输入输出
        output_inputs = numpy.dot(self.who, hidden_outputs)
        output_outputs = self.activation_function(output_inputs)
        
        #计算输出误差
        output_errors = targets-output_outputs
        
        #计算隐藏层误差
        hidden_errors = numpy.dot(self.who.T, output_errors)
        
        #更新who、wih
        self.who += self.lr * numpy.dot((output_errors * output_outputs * (1 - output_outputs)), numpy.transpose(hidden_outputs))
        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1 - hidden_outputs)), numpy.transpose(inputs))
        
    #使用神经网络进行预测
    def test(self, input_num):
        inputs = numpy.array(input_num, ndmin=2).T
        
        hidden_inputs = numpy.dot(self.wih, inputs)
        hidden_outputs = self.activation_function(hidden_inputs)
        
        output_inputs = numpy.dot(self.who, hidden_outputs)
        output_outputs = self.activation_function(output_inputs)
        
        return output_outputs

#定义输入
input_nodes = 784
hidden_nodes = 300
output_nodes = 10

learning_rate = 0.1

n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)

#加载训练集
training_data_file = open("mnist_data/mnist_train.csv")
training_data = training_data_file.readlines()
training_data_file.close()

#训练数据集
generation = 7

for i in range(generation):
    for record in training_data:
        all_values = record.split(',')
        #将输入规范到0.01到1
        inputs = (numpy.asfarray(all_values[1:])/255*0.99) + 0.01
        #将输出规范为除目标标签为0.99，其余均为0.01
        targets = numpy.zeros(output_nodes) + 0.01
        targets[int(all_values[0])] = 0.99
        n.train(inputs, targets)

#加载测试数据
test_data_file = open("mnist_data/mnist_test.csv", "r")
test_data = test_data_file.readlines()
test_data_file.close()

# 测试计分
scorecard = []

for record in test_data:
    all_values = record.split(',')
    # 给出正确标签
    correct_label = int(all_values[0])
    #将输入规范到0.01到1
    inputs = (numpy.asfarray(all_values[1:])/255*0.99) + 0.01
    #将输出规范为除目标标签为0.99，其余均为0.01
    outputs = n.test(inputs)
    label = numpy.argmax(outputs)
    if(label == correct_label):
        scorecard.append(1)
    else:
        scorecard.append(0)

# 计算神经网络的表现
scorecard_array = numpy.asarray(scorecard)
print('Performance = ',scorecard_array.sum()/scorecard_array.size)
```

## 参考文献

1. [英]Tariq Rashid.Python神经网络编程[M].北京：人民邮电出版社，2018



