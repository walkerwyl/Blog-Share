---
layout:     post
title:      "《Python 机器学习》第6章学习总结"
date:       2019-04-29
author:     "董国珍"
header-img: "img/post-bg-2015.jpg"
tags:
    - 数据分析
---



# 《Python 机器学习》第6章学习总结 
   

## 第6章 集成方法

  
集成方法来源于：如果模型之间近似相互独立，则多个模型联合的性能要优于单个模型

集成方法的关键：开发出一种可以生成大量近似独立的模型的方法，然后把它们集成起来

集成方法是由两层算法组成的层次架构。底层的算法叫作基学习器（base learner）

基学习器是单个机器学习算法，这些算法后续会被集成到一个集成方法中。 

常用的基学习器：二元决策树、支持向量机等，但从实用角度二元决策树的应用最为广泛
常用的上层方法：投票（bagging）、提升（boosting）和随机森林（random forests）

  
* 6.1 二元决策树
  
二元决策树就是基于属性做一系列的二元（是/否）决策。每次决策对应于从两种可能性中选择一个。每次决策后，要么引出另外一个决策，要么生成最终的结果。  
> * 用二元决策树进行预测：  
>  
> 决策过程持续进行，直到到达一个终止节点（即叶子节点）, 叶子节点给该行数据分配预测值（所有到达此节点的训练数据结果的均值）   
>
> 在二元决策树中，越是重要的变量越早用来分割数据（越接近决策树的顶端）
>
> 深度：从上到下遍历树的最长路径（所经过的决策的数目）  
> 深度为 1 的树又叫 作桩（stumps）  
> 深度控制二元决策树模型的复杂度  
>  
> * 训练一个二元决策树 （决策树的训练等同于分割点的选择 ）  
>  
> 分割点选择算法(目标是使预测值的误差平方最小)  
> 假设分割点已确定==》计算比较  
>
> ![avatar](/Blog-Share/img/1904/04/Doris/6.1.png)
>  
> 大概在数据的中心，可以明确地取到最小的误差平方。训练一个决策树需要穷尽地搜索所有可能的分割点来确定哪个值可以使误差平方和最小化
>   
> 决策树训练算法通常有一个参数来控制节点包含的数据实例最小到什么规模就不再分割。节点包含的数据实例太少会导致预测结果发生剧烈震荡
>  
> 过拟合：（二元决策树的参数（树的深度、最小叶节点规模等等）可以用来控制模型的复杂度）
> >
> > 表现：很难看出真实值与预测值之间的差别，预测值几乎完全跟随每一个观察值的变化；比较决策树中终止节点的数目与数据的规模（大量的数据单独占据一个终止节点）
> >
> > 控制过拟合：使用交叉验证（cross-validation）
> >
> 分类问题常用的评价标准：误分类错误，基尼不纯性度量，信息增益
>

##### 以下为三个主流的集成方法：

* 6.2 自举集成：Bagging 算法

> 自举集成算法使用叫作自举（bootstrap）的取样方法。自举取样通常用来从一个中等规模的数据集中产生取样统计  
> 一个（非参）自举取样是从数据集放回式地随机选择元素（自举取样可能会重复取出原始数据中的同一行数据）
>
> 存在两种误差：偏差和方差
> >
> > 偏差: 更多的数据点加入时，并不能减少的误差(克服方法:增加决策树的深度)
> >

* 6.3 梯度提升法（Gradient Boosting）

梯度提升法是基于决策树的集成方法，在不同标签 上训练决策树，然后将其组合起来。对于回归问题，目标是最小化均方误差，每个后续的决策树是在前面决策树遗留的错误上进行训练

> 在减少方差的同时，还可以减少偏差(不同于另两种方法)
>  
> 梯度提升法使用了梯度下降法（如果步长太大，优化过程就会发散而不是收敛；如果步长太小，则需要执行太多次迭代）
>
> 残差通常用于表示预测误差（在这里是：观测值减去预测值）  
> 在开始阶段，残差等于观测值（初始化预测值为0）
>
>问题属性之间存在强的相互依赖、相互作用时，梯度提升法只需要调 整决策树的深度
>

* 6.4 随机森林

随机森林在数据 集的子集上训练出一系列的模型。这些子集是从全训练数据集中随机抽取的。一种抽取方法就是对数据行的随机放回取样;另一种方法是每个决策树的训练数据集只是所有属性随机抽取的一个子集，而不是全部的属性。

> 随机森林：Bagging 加上随机选择的属性子集 
>  
> 随机森林是两个方法的结合，包括 Bagging 方法和属性随机选择方法。
>
